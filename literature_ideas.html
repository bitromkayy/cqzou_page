<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Literature & Ideas - ZOU CHENGQI Kay</title>
    <style>
        /* Inherit some styles from the main page for consistency */
        body {
            font-family: 'Segoe UI', Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f7f6;
            color: #333;
        }

        .container {
            max-width: 960px;
            margin: 20px auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);
        }

        header {
            text-align: center;
            padding: 30px 20px;
            background-color: #34495e; /* Consistent with main page nav bar */
            color: #ecf0f1;
            border-radius: 8px 8px 0 0;
            margin-bottom: 20px;
        }

        header h1 {
            margin: 0;
            font-size: 2.5em;
            color: #ecf0f1;
        }

        nav {
            background-color: #2c3e50; /* Slightly darker than header */
            padding: 10px 0;
            text-align: center;
            border-radius: 0 0 8px 8px;
            margin-bottom: 20px;
        }

        nav a {
            color: #ecf0f1;
            text-decoration: none;
            padding: 10px 20px;
            display: inline-block;
            transition: background-color 0.3s ease;
        }

        nav a:hover {
            background-color: #34495e;
            border-radius: 4px;
        }

        section {
            margin-bottom: 30px;
            padding: 20px;
            border: 1px solid #eee;
            border-radius: 6px;
            background-color: #fafafa;
        }

        h2 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
            margin-top: 0;
            margin-bottom: 20px;
        }

        h3 {
            color: #3498db;
            margin-top: 25px;
            margin-bottom: 10px;
        }

        p {
            margin-bottom: 15px;
            color: #555;
        }

        ul {
            list-style: disc;
            margin-left: 20px;
            color: #555;
        }

        ul li {
            margin-bottom: 8px;
        }

        footer {
            text-align: center;
            padding: 20px;
            margin-top: 30px;
            color: #777;
            font-size: 0.9em;
            border-top: 1px solid #eee;
        }
    </style>
</head>
<body>
    <header>
        <h1>Literature & Ideas</h1>
    </header>

    <nav>
        <a href="index.html">Back to Main Page</a>
    
    </nav>

    <div class="container">
        <section id="literature">
            <h2>My Literature Readings and Insights</h2>
            <p><strong> Living Document - Actively Updated</strong> [Last Snapshot: 2025/12/6]</p>
            <p>Neuroengineering is a highly interdisciplinary field that encompasses neuroscience, material science, imaging system design, machine learning, interpretable deep learning, and signal processing with feature extraction. To gain a deeper understanding of what I'm researching, I like reading papers that propose novel and interesting research perspectives and system architecture design for inspirations. Here I document some literature I've read and write down my thoughts during the reviewing. They are organized according to their research fields. </p>






            <h3>Neural Signal Decoding and Signal Processing</h3>
            <ul>
                <li>
                    <strong>“Deep learning for neural decoding in motor cortex” by Fangyu Liu et al.:</strong>
                    DOI: <a href="https://doi.org/10.1088/1741-2552/ac8fb5">10.1088/1741-2552/ac8fb5</a>
                    <br><small><strong>Notes: 1. </strong>How to compare deep learning model and human-generated simple hypothesis-driven model: if a human-generated simple hypothesis-driven model is less accurate than a deep learning model in the same task, the hypothesis-driven model likely has failed to capture important principles</small>
                    <br><small><strong>2. </strong> conclude 4 input feature normalization methods</small>
                    <br><small><strong>3. 5 * 2cv test: </strong> a statistical test to infer the statistical significance of differences in decoding performance among different models within a session</small>
                    <br><small><strong>4. </strong>use SHAP for interpretability analysis. The larger the absolute SHAP value, the more important the feature is</small>
                    <br><small><strong>5. </strong>ANN:use all inputs to process an output; LSTM: sequentially fed inputs to produce outputs</small>
                    <br><small><strong>6. </strong>For concurrent decoding (decode the position of the forelimb movement at that time), ANN (powerful non-linear-fitting capacity) > LSTM > Conventional ML models </small>
                    <br><small><strong>7. </strong>107 ms preceding time interval carries significant movement-related informations</small>
                    <br><small><strong>8. Spatiotemporal decoding: </strong>reconstruct images: each neuron is represented as a dot positioned at its center of mass and the brightness of the colered dot indicated the activity level of the corresponding neuron. The connectivity between a pair of neurons is represented by the thickness of the line connecting the two dots.</small>
                    <br><small><strong>9. </strong>Identity mapping by short connections to fix the shattered gradient problem (a significant challenge in deep learning where the gradient signal cannot effectively propagate backward through certain layers or operations during the backpropagation process)</small>
                    <br><small><strong>My Insight: 1. </strong>This work provides us a complete framework to decode neural signals from model design, data processing, data reconstruct to model performance evaluation, model comparison, interpretability analysis, and sensitivity analysis</small>
                    <br><small><strong>My Insight: 2. </strong>We may try to use GCN to substitute CNN because it can capture more spitial informations of neural signals</small>
                    <br><small><strong></strong></small>
                
                </li>


                <li>
                    <strong>“A Real-Time Framework for EEG Signal Decoding With Graph Neural Networks and Reinforcement Learning” by Htoo Wai Aung et al.:</strong>
                    DOI: <a href="https://doi.org/10.1109/TNNLS.2025.3558171">10.1109/TNNLS.2025.3558171</a>
                    <br><small><strong>Notes: 1. </strong>two primary categories of GCNs: spatial and spectral methods</small>
                    <br><small><strong>2. </strong>EEG_GLT can be used to optimize the adjacency matrix by calculating in different density levels (DOTA in adj_matrix construction)</small>
                    <br><small><strong>3. </strong>design a novel model architecture EEG_RL-Net, which is a combination of GNNs and RL, to classify EEG MI time point signals</small>
                    
                    
                    <br><small><strong>My Insight: 1. </strong>For strengthen the stability and improve accuracy, we can integrate RL into our deep learning model for neural decoding</small>
                
                    <br><small><strong></strong></small>
                </li>

                <li>
                    <strong>“The structural-functional-connectivity coupling of the aging brain” by Hui Zhang et al.:</strong>
                    DOI: <a href="https://doi.org/10.1007/s11357-024-01106-2">10.1007/s11357-024-01106-2</a>
                    <br><small><strong>Notes: 1. </strong>Structural connectivity matrix (SC): by counting the number of streamlines between all pairs of brain parcels. (DTI)</small>
                    <br><small><strong>2. </strong>Functional connectivity matrix (FC): by calculating the correlation coefficients between all pairs of brain parcels. (Resting-state fMRI)</small>
               
                    <br><small><strong>3. </strong>Structural connectivity matrix (SC): by counting the number of streamlines between all pairs of brain parcels. (DTI)</small>
                    <br><small><strong>3. </strong>Gordon's 12 functional brain clusters - a prior modules</small>
                    <br><small><strong>4. </strong>Structural-Functional-Connectivity Coupling (SFC): by calculating the Spearman-rank correlation coefficients between SC and FC matrix</small>
                    <br><small><strong>Intra-network SFC: </strong>SC and FC from the same functional brain clusters</small>
                    <br><small><strong>Inter-network SFC: </strong>SC and FC from different functional brain clusters</small>
                    <br><small><strong>My Insight: 1. </strong>For this work, we can train a multigraph GNN to carry out a regression task with different pathological, physiological, and other index as labels (e.g., age, edu, SVD score, and so on). 
                        We may construct graphs in which the nodes represent Gordon's 12 functional brain clusters. The edges correspond to the functional (FC) and structural (SC) connectivity between them, with the option to include SFC. For each node, its feature was defined as the sum of the corresponding column in the SC and FC matrices. 
                        For each edge, its feature was the direct connectivity value from these matrices. After the model training and validation, we can identify which functional brain clusters and which SC and/or FC edges contribute most to that index by interpretability analysis using GNNExplainer. A statistical analysis will be carried out on the results of the interpretability analysis for structural-functional-connectivity coupling analysis.</small>

                    <br><small><strong></strong></small>
                    <br>
               
                </li>

                <li>
                    <strong>“Evaluating fMRI preprocessing pipelines” by S.C. Strother:</strong>
                    DOI: <a href="https://doi.org/10.1109/MEMB.2006.1607667">10.1109/MEMB.2006.1607667</a>
                    <br><small><strong>Notes: 1. Cost function: </strong>measure the similarity of image volume in a time series to a reference volume </small>
                    <br><small><strong>2. </strong>Interpolation: Tradeoff between residual interpolation errors and speed</small>
                    <br><small><strong>3. </strong>Two preprocessing path (1): all-in-one analysis (data is completely preprocessed before data analysis); (2): summary statistic approach (individual-subject preprocessing and data analysis)</small>
                    <br><small><strong>My Insight: 1. </strong>This is a very useful guidelines for learning fMRI preprocessing</small>
                    <br><small><strong>My Insight: 2. </strong>Some software tools are listed in this paper for fMRI preprocessing</small>
                    <br><small><strong>My Insight: 3. </strong>For any preprocessing of signals including fMRI, we need to carry out Quality Control to visually assess the result of preprocessing steps to ensure the correctness of our preprocessing methods</small>
               
                    <br><small><strong></strong></small>
               
                </li>
                <li>
                    <strong>“Vicarious body maps bridge vision and touch in the human brain” by Nicholas Hedger et al.:</strong>
                    DOI: <a href="https://doi.org/10.1038/s41586-025-09796-0">10.1038/s41586-025-09796-0</a>
                    <br><small><strong>Notes: 1. </strong>What is the problem the article trying to address: how to simultaneously map somatosensory body part tuning and visual field tuning throughout the brain</small>
                    <br><small><strong>2. </strong>The crossmodal relationship between vision and touch is bidirectional</small>
                    <br><small><strong>3. </strong>Three hypothesis and predictions to test: (1) (the existence of <strong>endogenous</strong> somatotopic maps) the engagement of unimodal somatosensory brain regions even without exogenous sensory simulation; (2) visual inputs should engage multimodal neural sites with selective tuning tethered to both visual and somatosensory reference frames; (3) neural responses to visual input affording bodily interaction should be better explained by models incorporating selective tuning in both visual and somatosensory modalities instead of just visual tuning (what rules govern the relationship between somatosensory and visual tuning: (i) alignment to enviornmental statistics (or with the visual system's retional reference frame) (ii) a more category-selective level of alignment (somatosensory selectivity -predict-> visual selectivity))</small>
                    <br><small><strong>4. modelling the topographic structure of naturalistic brain activations </strong>reveals the computational motifs connecting visual and touch representations</small>
                    <br><small><strong>5. The first principle of this work: topographic organization -</strong> a shared principle of our sensory system</small>
                    <br><small><strong>6. Primary visual contex (V1): retionotopic map - </strong>neighboring regions -> neighboring locations of the visual field</small>
                    <br><small><strong>7. Primary somotosensory contex (S1): somototopic map - </strong>neighboring locations -> neighboring bodily locations</small>
                    <br><small><strong>8. How to apply the first principle to test our hypothesis and predictions: connection field - </strong>for every target voxel in the brain, estimate the spatial patterns on the source regions of V1 and S1 that best explain its BOLD dependent time-course</small>
                    <br><small><strong>9. Logic chain of how to build brain-wide tomographic mapping: </strong>the retinotopic organization of the visual system can be revealed from resting-state BOLD -> endegenous sensory-topographic structure recruited during spontaneous thought -> resting-state connectivity within S1 is organized by functional body-part boundaries -> visable brain-wide topographic mapping of the somatosensory system</small>
                    <br><small><strong>10. </strong>several orderly somatotopic gradients exist that mirror classical hallmarks of human somotosensory organization</small>
                    <br><small><strong>12. </strong>somatotopic maps' biased representations of body part -> putatively reflecting functional specialization</small>
                    <br><small><strong>13. </strong>The method to examine how connectivity captures the representational biases -> analyse the proportion of connectivity to each of these subfields</small>
                    <br><small><strong>14. </strong>challenge to author the BOLD fluctuations to specific mental content -> vary across individuals and time -> to examine how somatotopic network is influenced by structured naturalistic visual inputs -> methods: connective-field modelling</small>
                    <br><small><strong>15. </strong>analysis of video watching reveals somatotopically structured activation in extrastriate cortex</small>
                    <br><small><strong>16. </strong>question: how somatotopically structured activations in extrastriate cortex related to retinotopic activations more traditionally associated with this region -> methods: simultaneous estimation of somatosensory and visual connective fields -> enables quantificationof relative importance in explaining BOLD (analyse the balance of explained variance by visual and somatosensory connective fields) -> test the centrality of somatotopic responses in visual cortex</small>
                    <br><small><strong>17. </strong>multimodal topographic connectivity -> retinotopic and somatotopic</small>
                    <br><small><strong>18. </strong>finding: somatotopic processing would be pivotal in responses to naturalistic visual simulation in the lateral and dorsal, but not ventral visual system</small>
                    <br><small><strong>19. </strong>exogenous simulation while watching video -> a visually driven recruitment of the sensorimotor system resembling mirror neuron activations -> predict a predominant role of motor cortex rather than somatosensory cortex</small>
                    <br><small><strong>20. </strong>somatotopic connectivity explains BOLD signal variance during video watching -> possibility: visual cortex body-part tuning may be structured as orderly, somatotopic map-like arrangements on the cortical surface -> methods: project connective-field estimated somatotopic tuning onto this region -> suggest a hybrid organization: discrete field-based topography in core somatosensory regions and integrative cross-field tuning in higher-order partietal and visual areas</small>
                    <br><small><strong>21. </strong>problem: what is the principle that vision and somatosensation might br configured to connect to and recruit one another in the dorsolateral visual cortex -> much of the visual system is retinotopically organized -> alignment of the somatotopic and retinotopic reference frames -> hypothesis 1: reflect ecological coincidences between visual field and bodily positions supporting computation of enviornmental affordances -> topographic alignment may occur at a categorical level</small>
                    <br><small><strong>22. </strong>topographic alignment may occur at a categorical level -> somatotopic structure predicts visual body-part selectivity -> hypothesis 2: able to predict voxel-level visual selectivity to body parts from our somatotopic map -> use a pose-estimatiion algorithm -> the somatotopic map predict visual body part preference from the superior portion of FBA into the ventral portion of the EBA -> suggest at least two complementary multisenory alignment modes in visual cortex: visuospatial dorsally and semantic ventrally</small>
                    <br><small><strong>23. </strong>Somatotopic tuning: visual field locations -> more dorsally; visual body part -> more vertrally</small>
                    <br><small><strong>24. </strong>interpreting the findings requires acknowledging fundamental disjuctures between vision and somatosensation</small>、
                    <br><small><strong>25. </strong>main methodology in their study: alignment of visual and somatosensory maps -> a fundamental principle of brain organization</small>
                    <br><small><strong>26. </strong>preferences for positions on the visual field can be estimated by referencing the estimated connective-field V1 positions against the retinotopic map of V1</small>
                    <br><small><strong>27. </strong>How to design matrix and built connective field: Laplace-Beltrami operator (LBOEs) -> yield a finite family of real-valued functions that are intrinsic to the surface space, orthogonal and ordered according to spatial scale -> approximate any arbitrary spatial pattern on the surface (a connective-field) through a linear combination of LBOEs</small>
                    <br><small><strong>28. </strong>How to estimate connective-field: the dot production of its estimated b*i and the corresponding spatial profiles si reveals a spatial map of the imporatance of each vortex on S1 and V1 in explaining the voxels response -> "connective-field"</small>
                    <br><small><strong>29. </strong>Alternative model -> test the organizing influence of body part fields -> compare fully connective-field model to 4 alternative connective field models (corresponding to the lower limb, trunk, upper limb and face field) -> comparing delta_R2 and 0</small>
                    <br><small><strong>My Insight: 1. </strong>All the experiment designs in this work follow this general closed-loop logic chain, which I will learn to follow in my future research: (i) start from the first principle of our sensory system: topographic organization; (ii) according to previous findings, formulate hypothesis based on the topographic organization to try to explain thes findings; (iii) build connectivity maps and/or connection field to apply the first principle to test our hypothesis and predictions; (iv) implement verified models, algorithm, and statistical methods to analyse the hypothesis-related data; (v) according to quantitative analysis of statistics and model predictions, make a conclusion; (vi) extract new findings for next hypothesis</small>
                    <br><small><strong>My Insight: 2. </strong>In brain signal design, local information and connective information both count! To analysis the connective information, alignment of topographic maps is an effective method</small>
                    <br><small><strong>My Insight: 3. </strong>Brain modeling is not just fitting the brain signals with an existing ML or DL models. Brain modeling starts from a first principle of our sensory system, formulates rational hypothesis, and design or implement relevant ML/DL model to test the hypothesis. Finally, try to use first principle and physiological knowledge to interprete our finding.</small>


                    
                    
                    
                    
                
                    <br><small><strong></strong></small>
                </li>

                <li>
                    <strong>“Connectome caricatures remove large-amplitude coactivation patterns in resting-state fMRI to emphasize individual differences” by Htoo Wai Aung et al.:</strong>
                    DOI: <a href="https://doi.org/10.1038/s41593-025-02099-7">10.1038/s41593-025-02099-7</a>
                    <br><small><strong>1. </strong>caricaturing: a method for projecting resting-state data onto a subspace orthogonal to a manifold of coactivation patterns estimated from task fMRI data</small>
                    <br><small><strong>2. </strong>First principle in this study: high-amplitude coactivation -> shared features within population; removing these shared signals (hidden signal) -> intrinsic different architecture of the individual-subject</small>
                    <br><small><strong>3. </strong>Two theoretical frameworks exsit to emphasize individual differences in functional connectivity: (1) 'Spotlight' approach: blur the irrelevant features while retaining and enriching relevant ones -> Task-induced change in fMRI: consistently modulating the coactivations across individuals; (2) 'caricature' approach: exaggerate "the most prominent features of each individual"</small>
                    <br><small><strong>4. </strong>resting-state fMRI signal - dominant coactivation patterns = a hidden signal -> characterizing the brain's intrinsic function architecture</small>
                    <br><small><strong>5. </strong>How to implement caricaturing: project the resting-state fMRI time series away from a manifold of task coactivation patterns estimated from the task-based time series -> (1) construct covariance matrix and calculate eigenvectors; (2) create a projection matrix to project an fMRI time point into a subspace that is orthogonal to the eigenvectors explaining the most variance -> top eigenvectors represent the low-dimensional coactivation patterns that recur accross tasks; (3) multiply this projection matrix by each tima point to create the 'hidden signal' time series; (4) correlate pairs of time series to create caricature connectomes</small>
                    <br><small><strong>6. </strong>use MCA (minor component analysis) instead of PCA (principle component analysis):(1) PCA: use top components; MCA extract component accounting for the smallest variance; (2) PCA may overlook weak signal; MCA is usrful when desired signal in weak and less prominent patterns in data</small>
                    <br><small><strong>7. </strong>investigate multivariate reliability -> determine whether changes in connectome similarity had a wider effect on the data</small>
                    <br><small><strong>8. </strong>Reliability: a function of within- and between-individual similarity -> higher ratio between the two -> higher reliability</small>
                    <br><small><strong>9. </strong>multivariate reliability: whole-brain-level (improved by caricaturing); univariate reliability: edge-level (decreased with caricaturing)</small>
                    <br><small><strong>10. </strong>multivariate provides an upper limit for prediction -> caricaturing imporve multivariate reliability -> improve prediction performance</small>
                    <br><small><strong>11. </strong>multivariate reliability is separable from univariate reliability</small>
                    <br><small><strong>12. </strong>Limitations: (1) Possibility of data loss: it is unclear how much task-relevant information is removed from the resting-state data; (2) only use linear method to define the manifold; (3) only use correlation to measure distance ->other matrics may yield slightly different results; (4) the observed prediction imporvement is modest and small -> combine with other advancements for further improvements; (5) note whether caricaturing is biased by various demographic factors; (6) as caricaturing uses MCA, caricatured data may contain a higher proportion of unstructured noise than standard connectomes.</small>

                    <br><small><strong>(Actively studying the Methods section and preparing for result reproduction)</strong></small>


                    <br><small><strong>My Insight: 1. </strong>Datasets which would be useful in our future research: HCP (Human connectome project), CNP (Consortium for Neuropsychiatrix Phenomics), TRT (the Yale Test-Retest)</small>
                    <br><small><strong>My Insight: 2. </strong>For prediction related to individual difference, caricaturing can act as an enhancement technique for signal preprocessing</small>
                    <br><small><strong>My Insight: 3. </strong>The logic chain in the experiment design: (1) start from first principle: high-amplitude coactivation -> shared features within population; removing these shared signals (hidden signal) -> intrinsic different architecture of the individual-subject; (2) build a caricaturing framework to remove dominant coactivation patterns and emphasize hidden signals that characterize the individual difference; (3) use a series of experiments to verify the framework and the advantages of coactivation-removed signal</small>
                    <br><small><strong>My Insight: 4. </strong>An autoencoder-based DL model could be implemented for the caricaturing process, where the encoder can process the fMRI data into </small>
                
                    <br><small><strong></strong></small>
                </li>



                <li>
                    <strong>“Prediction of neural activity in connectome-constrained recurrent networks” by Manuel Beiran & Ashok Litwin-Kumar:</strong>
                    DOI: <a href="https://doi.org/10.1038/s41593-025-02080-4">10.1038/s41593-025-02080-4</a>
                    <br><small><strong>1. </strong>connectome-constrained neural network: 'student' network: trained to reproduce the activity; 'teacher': ground truth -> same synaptic weights but different biophysical parameters -> reflecting uncertainty in neuronal and synaptic properties</small>
                    <br><small><strong>2. </strong>a major challenge for connectome-constrained models: uncertainty in biophysical parameters that affect neural dynamics</small>
                    <br><small><strong>3. </strong>Problem: training a connectome-constrained student network to generate the task-related readout of the teacher does not always produce consistent dynamics in the teacher and student -> Reason: multiple combinations of signal-neuron parameters can equivalently solve the same task (The Problem of Degeneracy)</small>
                    <br><small><strong>4. </strong>Method to break the degeneracy: combine the connectivity constrains with recording of the activity of a subset of neurons -> the minimum number of recording depends on the dimensionality of the network dynamics instead of the total number of neurons</small>
                    <br><small><strong>5. </strong>'stiff' parameters: strong effect on neural dynamics; 'sloppy' parameters: weak effects</small>
                    <br><small><strong>6. </strong>Teacher: used as a proxy for a neural system whose connectomes has been mapped and whose output for a neural activity can be recorded</small>
                    <br><small><strong>7. </strong>Student: trained to generate the same readout as the teacher -> succesfully reproduce the teacher's readout but have error in the neural activity of the student</small>
                    <br><small><strong>8. </strong>The knowledge of synaptic weights and task output is not always enough to predict the activity of signal neurons in recurrent networks -> problem: a degenerate space of solutions, with different combinations of single-neuron gains and biases, that solve the same task -> proposed solution: instead of recording only task-related readout activity, record the activity of a subset of neurons in the teacher network</small>
                    <br><small><strong>9. </strong>Two possibilities about the required number of recorded neurons: (1) fixed fraction of the total number of neurons in the network; (2) determined by properties of the network dynamics -> by experiment, determined by dimensionality of the neural dynamics</small>
                    <br><small><strong>10. </strong>Why consider robustness to model mismatch: we don't know the ground true activation function and parameterized biophysical process -> analyse the robustness when (1) model mismatch; (2) noise</small>
                    <br><small><strong>11. 'stiff' modes:</strong> (1) have the greatest effect on the loss; (2) are learned most quickly -> determinedby the corresponding singular value of matrix A</small>
                    <br><small><strong>12. 'sloppy' modes:</strong> its associated singular value is zero -> that parameter differences between students and teachers along that mode produce no differences in neural dynamics</small>
                    <br><small><strong>13. </strong>the parameter space is 2d -> we can visualize the loss function for the full network across a grid of parameters</small>
                    <br><small><strong>14. </strong>The sloppy mode is different for each of the two recorded neurons -> mismatch in unrecorded activity -> recording from a single neuron constrains activity along only one dimension of the two dimensional activity space defined by the rank-two synaptic weight matrix</small>
                    <br><small><strong>15. </strong>The most informative neuron i is the one whose corresponding row A_i overlaps most with the weighted left singular vectors of A</small>
                    <br><small><strong>(Actively studying the Methods section and preparing for result reproduction)</strong></small>


                    
                    <br><small><strong>My Insight: 1. </strong>The first principle of this work: Synaptic connectivities can constrain neural dynamics to a low-dimensional representation or manifold, enabling the prediction of global activity from a small subset of recorded neurons</small>
                
                    <br><small><strong>My Insight: 2. </strong>For neural signal decoding, because neural signal docoding commonly has many biophysiological and synpatic parameters, we need multiple constrains instead of just output constrains if we can out model learn latent neural activity from the raw neural signals.</small>

                    <br><small><strong>My Insight: 3. </strong>A 3-steps logic chain in scientific Argumentation: Step 1 - Model Design & Core Finding ('student-teacher' model design -> identify limitations (degeneracy) -> introduce sparse recordings to break degeneracy -> dimensionality vs size -> robustness for model mismatch); Step 2 - Empirical/Animal Validation (Drosophila Larva + Adult Drosophila + Zebrafish); Step 3 - theoretical analysis & application (analytical linear model -> 'stiff' vs 'sloppy' -> loss landscape -> optimal selection of single neurons)</small>
               
                    <br><small><strong>My Insight: 4. </strong>Don't just focus on the decrease in total loss. Use the Hessian matrix or SVD to analyse the curvature of the loss</small>
                    <br><small><strong>My Insight: 5. </strong>In degenerate network, the optimal solution is often not a optimised global point, but a flat valley-shaped region. -> So we need use other techniques to constrain the parameters and training.</small>

                </li>


                </li>




            </ul>


            <h3>Interpretable Deep Learning Architecture</h3>
            <ul>
                <li>
                    <strong>“BrainGNN: Interpretable Brain Graph Neural Network for fMRI Analysis” by Xiaoxiao Li et al.:</strong>
                    DOI: <a href="https://doi.org/10.1016/j.media.2021.102233">10.1016/j.media.2021.102233</a>
                    <br><small><strong>Notes 1: </strong>Graph Nodes: ROIs; Graph Edges: Connectivity between two ROIs</small>
                    <br><small><strong>2: brainGNN Architecture: </strong>graph input -> ROI-aware Graph Convolution Layer 1 (Ra-GConv 1) -> Pooling Layer 1 -> Ra-GConv 2 -> Pooling Layer 2 -> Readout Layer (Global Pooling Layer and Concatenate) -> MLP -> Classes</small>
                    <br><small><strong>3: Ra-GConv Layer: </strong>Soft Community Assignment -> Conditional Kernel Embedding -> Aggregate Node Information from Neighbors -> Update Node Representation</small>
                    
                    <br><small><strong>4: ROI Pooling Layer: </strong>Start from node embedding from Ra-GConv -> Projectiong On Pooling Vector -> Node Score Assignment -> Keep High-Score Nodes</small>
                
                    <br><small><strong>5: </strong>Location information is represented by one-hot encoding because the nodes in the brain are aligned well</small>
                
                    <br><small><strong>6: Concatenation: </strong>z_l = mean H_l || max H_l , z = z_1 || z_2 || ... || z_l</small>
                    <br><small><strong>7: Loss Function: </strong>Cross Entropy Loss + Unit Loss (to avoid indentifiability problem, i.e. the multiple parameters generate the same distribution of the observed data) + Group-level Consistency Loss (GLC Loss) (to force GNN to select similar ROIs in a R-pool layer for different input instances) + TopK Pooling Loss (TKP Loss) (to encourage reasonable node selection: selected: 1; unselected: 0)</small>
                    <br><small><strong>My Insight: 1. </strong>The work provides us a framework to construct our brain neural signals as graphs and use an interpretable BRAINGNN to predict some index</small>
                
                    <br><small><strong>My Insight: 2. </strong>Position is important because Ra-GConv will use position information to learn a customized kernel (weight matrix) for convolution</small>
             
                    <br><small><strong></strong></small>
                </li>

                <li>
                    <strong>“Interpretable deep learning for deconvolutional analysis of neural signals” by Bahareh Tolooshams et al.:</strong>
                    DOI: <a href="https://doi.org/10.1016/j.neuron.2025.02.006">10.1016/j.neuron.2025.02.006</a>
                    <br><small><strong>Notes 1: </strong>Why we need DUNL: to decompose neural activity into overlapping and nonoverlapping components that capture heterogeneity in the population</small>
                    <br><small><strong>2: </strong>The parameters are shared between encoder and decoder</small>
                    <br><small><strong>3: </strong>The latend code: represent estimated stimulus onsets and strength of the neura; response to that stimulus</small>
                    <br><small><strong>4: </strong>The code we input is a mask for code support</small>
                    <br><small><strong>5: </strong>In this model, kernel H is the only parameter to be learnt</small>
                    <br><small><strong>My Insight: 1. </strong>In this framwork, we most important part is that by convolution, we can learn the underlying neural activity (code) by inputting a detectable neural signals. In true neural physiology, each one of these underlying neural activities (codes) can produce a series of detectable neural signals with different kernels. To make the model adapt to more realistic neural signals, we can propose a 2DDUNL based on DUNL, where a series of neural signals will be deconvoluted with different kernels to get the same neural activity code. By convoluting the code with different kernels, we can reconstruct the detectable neural signais.</small>
                    <br><small><strong>My Insight: 2. </strong>By designing a correlation loss, if we have real but non-sparse code, the DUNL can also learn the dense code.</small>

                    <br><small><strong></strong></small>
                </li>
                


                
                <li>
                    <strong>“KerGNNs: Interpretable Graph Neural Networks with Graph Kernels” by Aosong Feng et al.:</strong>
                    DOI: <a href="https://doi.org/10.1609/aaai.v36i6.20615">10.1609/aaai.v36i6.20615</a>
                    <br><small><strong>Notes: 1: </strong>KerGNN: integrate graph kernels into the message passing process of GNNs</small>
                    <br><small><strong>2: 3 steps to generate node embeddings for MPNNs: </strong>(1) receiving message from its neighbors; (2) aggregating messages; (3) updating its own features to encode the local information</small>
                    <br><small><strong>3: </strong>Problem: MPNNs are limited by WL (Weisfeiler-Lehman) kernels</small>
                    <br><small><strong>4: </strong>kerGNN: a sub-graph-based node aggregation algorithm</small>
                    <br><small><strong>5: </strong>Graph kernels are proposed to solve the problem of assessing the similarity between graphs</small>
                    <br><small><strong>6: </strong>Graph kernels: (1) simple node-pair kernel (2) random walk kernels (count the number of walk that two graphs have in common)</small>
                    <br><small><strong>7: </strong>For graphs without node features, we can use degree for substitution</small>
                    <br><small><strong>8: </strong>What is subgraph: the vertex-induced subgraph formed from a node and all its 1-hop neighbors</small>
                    <br><small><strong>9: </strong>How to carry out subgraph-based aggregation: extract subgraph -> compare against a set of learnable graph filters -> measure the topological similarity -> according to the similarity, update new node feature maps</small>
                    <br><small><strong>My Insight: 1. </strong>This architecture is meaningful and interpretable because it trains a set of visuallizable pattern templetes (graph filters), which are the features the model try to search in the input data.</small>
                
                    <br><small><strong>My Insight: 2. </strong>We can try to use KerGNN to carry out neural signal decoding tasks. For neural signals, the graph filters mean particular neural connectivity pattern which is related to pathological or psychological index we want to study. Beacuse we can directly visuallize the graph filters the model learnt, we do not need post-experimental interpretability analysis (e.g., using GNNExplainer).</small>
            
                    <br><small><strong></strong></small>
                </li>

                

            </ul>


            <h3>Deep Learning Application in Neuroengineering</h3>
            <ul>
                <li>
                    <strong>“Applications of interpretable deep learning in neuroimaging: A comprehensive review” by Lindsay Munroe et al.:</strong>
                    DOI: <a href="https://doi.org/10.1162/imag_a_00214">10.1162/imag_a_00214</a>
                    <br><small><strong>Notes: 1: </strong>General applications of DL in neuroimaging: segmentation, super-resolution, image synthesis, and classification</small>
                    <br><small><strong>2: </strong>Challenge of interpretable DL (iDL): difficult to assess the quality of explanations beacause the ground trues of the explanations are typically unavailable.</small>
                    <br><small><strong>3: Fidelity: </strong>the extent to which explanations reflect the inner working of the DL model -> feature-removal</small>
                    <br><small><strong>4: Robustness: </strong>the stability of the model explanations under different modelling conditions</small>
                    <br><small><strong>5: 2 main categories of iDL methods: </strong>post-hoc (reverse engineering, e.g. GNNExplainer, SHAP, ...) and intrinsic (incorporate interpretable components into model architecture while designing, e.g., DUNL and kerGNN) </small>
                    <br><small><strong>6: Post-huc methods: (1) </strong>Perturbation-based methods: alter input features and measure change of output. (Occlusion, Meaningful Perburbations, Local Interpretable Model-Agnostic Explanations (LIAE), Swap Test, Permutation Feature Importance). Advantages: easy to implement and understand; do not require network and gradient. Disadvantages: computationally intensive and time-consuming; distribution shift for change in input.</small>
                    <br><small><strong>(2) </strong>Gradient-based methods: compute the partial derivative of an output with respect to each input features. Need backpropagation. (Vanilla Gradient, Grad x Input, SmoothGrad, Integrated Gradients) Advantages: fast to run and easy to understand. Disadvantages: shattered gradients, saturation problem, gradient map is less able to discriminate between classes.</small>
                    <br><small><strong>(3) </strong>Backpropagation-based methods: apply rules to map the output score back to the input features to assign feature relevance. (Guided Backpropagation, Layer-wise Relevance Propagation (LRP), GNNExplainer).</small>
                    <br><small><strong>(4) </strong>Class activation maps: highlight image regions used by the final layer of a CNN to classify the input image. Gradient-Weighted Class Activation Maps (Grad-CAM): extent CAM to all CNNs to obviate the need for a GAP layer. Advantages: easy to implement and widely available. Disadvantages: heatmaps are coarse - low resolution because of upsampling.</small>
                    <br><small><strong>(5) </strong>Weight Analysis: analyze the weights of the trained network. (Network Dissection)</small>
                    <br><small><strong>7: Intrinsic methods: (1) </strong>Disentangled latent spaces: Input -> a learnable representation (latent space). (e.g. Autoencoder and Capsule Network) Advantages: provide some control for image generation to the end user. Disadvantages: the generative factors may not be inherently independent, by constraining the latent representations to must be independent, some information will lose. Come at the expense of performance. </small>

                    <br><small><strong>(2) </strong>Interpretable hybrid models and interpretable intermediate features: A hybrid model has two components: NN (neual network) + NN or NN + ML. The first NN is for processing intermediate feature representations, which can be understood by humans and act as model explanations. Advantages: can be designed so the intermediate features are suited for a particular application. Disadvantages: need careful design and take a long time to develop</small>
                    <br><small><strong>(3) </strong>Interpretable generative models: learn to generate modifications to the input image so that the modified image appears to belong to a different class. The modifications can be used as explanations. Advantages: capable of capturing more meaningful class-discriminative features. Disadvantages: high computational power needed and difficult to train</small>
                    <br><small><strong>(4) </strong>Deep structural causal models: estimate causal effects by simulating population-level interventions. Endogenous Variables + Exogenous or Noise Variables. Advantages: control for confounders. Disadvantages: should be constructed carefully from domain knowledge and impossible to obtain ground truth data.</small>
                    <br><small><strong>(5) </strong>Attention mechanisms: learn a heatmap over the inputs, features, ou channels of the neural network. Weight data to emphasise key features. 4 main types of attention: channel attention (assign a weight to each filter), spitial attention (extract important information in the image domain or across the spatial dimensions of a feature map), non-local attention (capture long-range dependencies by computing interactions between any two positions), and self attention (tokenization + QKV). Advantages: differentiable objective and easily trainable with gradient descent.</small>

                    <br><small><strong>My Insight: 1. </strong>This work can act as a guideline for us to design an interpretable model architecture and carry out interpretability analysis after model performance evaluation.</small>
                    <br><small><strong>My Insight: 2. </strong>In my opinion, designing a intrinsic interpretable model which is built upon our prior knowledge and physiological hypothesis is preferred beacause this model actually mimics the real physiological process in our brain (if the performance of the model is acceptable, we can use this interpretable model as a part of "silicon brain")</small>

                    <br><small><strong></strong></small>
                </li>

                <li>
                    <strong>“Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond” by Xuhong Li et al.:</strong>
                    DOI: <a href="https://doi.org/10.1007/s10115-022-01756-8">10.1007/s10115-022-01756-8</a>
                    <br><small><strong></strong></small>
                    <br><small><strong></strong></small>
                    <br><small><strong></strong></small>
                    <br><small><strong></strong></small>
                </li>

                <li>
                    <strong>“Dissociating language and thought in large language models” by Kyle Mahowald et al.:</strong>
                    DOI: <a href="https://doi.org/10.1016/j.tics.2024.01.011">10.1016/j.tics.2024.01.011</a>
                    <br><small><strong></strong></small>
                    <br><small><strong></strong></small>
                    <br><small><strong></strong></small>
                    <br><small><strong></strong></small>
                </li>


                <li>
                    <strong>“Joint modelling of brain and behaviour dynamics with artificial intelligence” by Mackenzie Weygandt Mathis & Alexander Mathis :</strong>
                    DOI: <a href="https://doi.org/10.1038/s41583-025-00996-1">10.1038/s41583-025-00996-1</a>

                    <br><small><strong>1. Three main broad classes of artificial intelligence in brain modelling: </strong>Discriminative, generative, and contrastive</small>
                    <br><small><strong>2. Discriminative: </strong>directly decoding behaviour from neural data</small>
                    <br><small><strong>3. Generative: </strong>learning to predict spike trains via reconstruction</small>
                    <br><small><strong>4. Contrastive: </strong>learning latents without reconstruction data</small>
                    <br><small><strong>5. Major advances in neural recording technologies: </strong>large-scale electrophysiology, calcium imaging and neuromodulatory tagging</small>
                    <br><small><strong>6. Major advances in behaviour measurement tools: </strong>high-resolution videa, inertial sensors and pose estimation techniques</small>
                    <br><small><strong>7. AI encompasses </strong>Machine Learning (ML), Deep Learning (DL), and Agent-based Systems</small>
                    <br><small><strong>8. The goal of AI models: </strong>solve challenging perception and decision-making problems</small>
                    <br><small><strong>9. Encoder: </strong>domain-specific feature engineering</small>
                    <br><small><strong>10. Decoder: </strong>trainable classification</small>
                    <br><small><strong>11. State-space models: </strong>Input -> h(t) = Ah(t-1) + Bx(t) A: state matrix B: input matrix -> hidden layer h(t) -> y(t) = Ch(t) + Dh(t) C: output matrix D: feedthrough</small>
                    <br><small><strong>12. The loss function shapes what the model learns by defining success</strong></small>
                    <br><small><strong>13. What is brain-behaviour modelling: modelling the joint distribution P(behaviour, neural data) </strong>-> four classes -> (1) decoding models: study P(behaviour | neural data); (2) Encoding models: study P(neural data | behaviour); (3) Latent models: study P(neural data); (4) joint models: model P(behaviour, neural data)</small>
                    <br><small><strong>14. Variational autoencoder (VAE): </strong>probabilistic generative framework to model data x from latent variables z</small>
                    <br><small><strong>15. Identifiability: </strong>A representation z=f(x) is indentifiable if model recovers the true latent variables</small>
                    <br><small><strong>16. Non-linear ICA: </strong>x=f(z) f is a non-linear, invertible function. z are statistically independent</small>
                    <br><small><strong>17. Contrastive learning -> information noise-contrastive estimation (InfoNCE) Loss</strong></small>
                    <br><small><strong>18. Goal of brain-behaviour modelling: </strong>(1) Engineering perspective: build brain-machine interfaces (BMIs); (2) Scientific perspective: 'digital twin' -> -capture the conputational principles and dynamical processes underlying neural function; -focus on biological principles rather than exact replication; (3) to test specific hypothesis about neural representations; (4) exploratory discovery: uncover novel patterns, cell types or computational motifs</small>


                    <br><small><strong>(Actively studying and updating)</strong></small>
                    <br><small><strong>My Insight: 1. </strong>When we choose what models will be implemented (Discriminative, generative, or contrastive) , we need to think about that 'what is the goal for brain-behaviour modelling'. When building the model architecture, we need to take physiology-based first principle of our hypothesis into account.</small>
                    <br><small><strong></strong></small>
                    <br><small><strong></strong></small>
                </li>
            </ul>



            <h3>Radiology and Imaging Technologies</h3>
            <ul>

                <li>
                    <strong>“Simultaneous EEG-fMRI for Functional Neurological Assessment” by Giulia Mele et al.:</strong>
                    DOI: <a href="https://doi.org/10.3389/fneur.2019.00848">10.3389/fneur.2019.00848</a>
                    <br><small><strong>Notes 1: </strong>Brain electrical activity is derived from the synchronizations of a pool of cortial neurons (pyramidal cells)</small>
                    <br><small><strong>2: </strong>the potentials that appear within the 100ms post stimulus are usually due to the nature of stimulus itself (artifact), while the subsequent components reflect the cognitive processes related to the perception of the stimulus</small>
                    <br><small><strong>3: </strong>BOLD is an indirect measure of neuronal activity</small>
                    <br><small><strong>4: </strong>fMRI: high spatial resolution; EEG: high temporal resolution</small>
                    <br><small><strong>5: </strong>Precautions in EEG-fMRI: (1) Gradient echo-echo planer protocol; (2) extensive safety testing with temperature sensors</small>
                    <br><small><strong>6: </strong>Simultaneous resting-state EEG-fMRI is fundamental for underling the variability of brain activity and above all to define the structures involved in the triggering EEG waves in resting state</small>
                    <br><small><strong>7: </strong>theta-alpha low frequency oscillations are linked to the functional activation of a network involving the hippocampus, the striatum, and the prefrontal cortex</small>
                    <br><small><strong>8: </strong>EEG-fMRI analysis methods: symmetric analysis (the simultaneous analysis of EEG and fMRI signal) and integrated analysis (use one signal to understand and validate another)</small>
                    <br><small><strong>My Insight: 1. </strong></small>For task-based simultaneous EEG-fMRI, we can use neuromodulation to stimulate a particular brain region and use simultaneous EEG-fMRI to monitor the brain response to the stimulation with high spatiotemporal resolution. This can help us understand the causal relationship between brain region function and cognitive process. i.e. We can build a simultaneous real time EEG-fMRI neurofeedback system
                    
                </li>

                <li>
                    <strong>“High temporal resolution functional MRI using parallel echo volumar imaging” by Cécile Rabrait MS et al.:</strong>
                    DOI: <a href="https://doi.org/10.1002/jmri.21329">10.1002/jmri.21329</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>


            </ul>

            <h3>Brain Computer Interface (hardware)</h3>
            <ul>
                <li>
                    <strong>“Real-time BCI system design to control arduino based speed controllable robot using EEG” by Swagata Das et al.:</strong>
                    DOI: <a href="https://doi.org/10.1007/978-981-13-3098-8">10.1007/978-981-13-3098-8</a>
                    <br><small><strong>Notes: </strong>This is a useful book to teach us how to build a BCI system to accomplish Arduino control from hardware prepration to software and AI implementation</small>
                </li>


            </ul>


            <h3>Brain Computer Interface (AI and software)</h3>
            <ul>
                <li>
                    <strong>“A Deep-Learning Empowered, Real-Time Processing Platform of fNIRS/DOT for Brain Computer Interfaces and Neurofeedback” by Yunjia Xia et al.:</strong>
                    DOI: <a href="https://doi.org/10.1109/TNSRE.2025.3553794">10.1109/TNSRE.2025.3553794</a>
                    <br><small><strong>Notes 1: </strong>Advantages of fNIRS over EEG: (1) higher spatial resolution; (2) potenital higher tolerance to motion artifacts</small>
                    <br><small><strong>2: </strong>Diffuse Optical Tomograph (DOT) employs an array of multiple near-infrared light sources and detectors at different source-detector separations - overlapping spatial sampling to reconstruct 3D images</small>
                
                    <br><small><strong>3: </strong>Challenges for fNIRS-DOT: (1) require a baseline, which is difficult to establish in real-time measurements; (2) real-time processing is hindered by the absence of prior information and delays</small>
                    <br><small><strong>4: </strong>typical fNIRS-DOT imaging pipeline: preprocessing - forward modeling - inverse problem solving</small>
                    <br><small><strong>My Insight 1: </strong>For real-time control of BCI, because of absence of baseline, pre-trained deep learning models for denoising and articate correction (e.g., Denoising autoencoder (DAE)) is an useful methods for real-time preprocessing</small>
                    <br><small><strong>My Insight 2: </strong>For real-time BCI, the biggest challenge we face is delay. How to decrease the delay into an acceptable range is an important issue we need to consider. To minize the delay while maintaining the accuracy of BCI, pre-trained and real-time fine-tunable (just before the using of BCI) deep learning system is need to be explored.</small>

                </li>
                
                <li>
                    <strong>“EEG-based brain-computer interface enables real-time robotic hand control at individual finger level” by Yidan Ding et al.:</strong>
                    DOI: <a href="https://doi.org/10.1038/s41467-025-61064-x">10.1038/s41467-025-61064-x</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>
                
                <li>
                    <strong>“A review of critical challenges in MI-BCI: From conventional to deep learning methods” by Zahra Khademi et al.:</strong>
                    DOI: <a href="https://doi.org/10.1016/j.jneumeth.2022.109736">10.1016/j.jneumeth.2022.109736</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>

                <li>
                    <strong>“Advancing BCI with a transformer-based model for motor imagery classification” by Wangdan Liao et al.:</strong>
                    DOI: <a href="https://doi.org/10.1038/s41598-025-06364-4">10.1038/s41598-025-06364-4</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>


             
            
            </ul>

    

            <h3>Neuroscience (Anatomy and Physiology)</h3>
            <ul>


                <li>
                    <strong>“Invariant neural dynamics drive commands to control different movements” by Vivek R. Athalye et al.:</strong>
                    DOI: <a href="https://doi.org/10.1016/j.cub.2023.06.027">10.1016/j.cub.2023.06.027</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>
            </ul>


            <h3>Computational Neuroscience (Computational Models, For Simulation Data Acquisition)</h3>
            <ul>
                <li>
                    <strong>“Benchmarking Deep Jansen-Rit Parameter Inference: An in Silico Study” by Tilwani, Deepa and O'Reilly, Christian:</strong>
                    Link: <a href="https://arxiv.org/html/2406.05002v1">https://arxiv.org/html/2406.05002v1</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>


                <li>
                    <strong>“EEG–fMRI Bayesian framework for neural activity estimation: a simulation study” by Pierpaolo Croce et al.:</strong>
                    DOI: <a href="https://doi.org/10.1088/1741-2560/13/6/066017">10.1088/1741-2560/13/6/066017</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --> </strong></small>
                </li>


            </ul>

            <h3>Materials</h3>
            <ul>

                <li>
                    <strong>“Properties and Applications of PDMS for Biomedical Engineering: A Review” by Ines Miranda et al.:</strong>
                    DOI: <a href="https://doi.org/10.3390/jfb13010002">10.3390/jfb13010002</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>


                <li>
                    <strong>“Soft, conformal PDMS-based ECoG electrode array for long-term in vivo applications” by Hyunmin Moon et al.:</strong>
                    DOI: <a href="https://doi.org/10.1016/j.snb.2023.135099">10.1016/j.snb.2023.135099</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>

            </ul>


            <h3>Animal Neuroscience Experiment and Experimental Paradigm Design</h3>
            <ul>
                <li>
                    <strong>“Neural Circuits Underlying Visually Evoked Escapes in Larval Zebrafish” by Timothy W. Dunn et al.:</strong>
                    DOI: <a href="https://doi.org/10.1016/j.neuron.2015.12.021">10.1016/j.neuron.2015.12.021</a>
                    <br><small><!-- [Notes to be added in the next update] --><strong></strong></small>
                </li>
                


                <li>
                    <strong>“Guidelines for the care and use of mammals in neuroscience and behavioral research” by National Research Council (US) Committee on Guidelines for the Use of Animals in Neuroscience and Behavioral Research
:</strong>
                    DOI: <a href="https://doi.org/10.17226/10732">10.17226/10732</a>
                    <br><small><strong>Notes: </strong>This guidelines provides us a systematic, ethical, and scientifically rigorous framework for designing neuroscience animal experiments</small>
                </li>
                


            </ul>




            

        </section>



        <section id="dataset">
            <h2>Open-source Dataset</h2>
            <p>Here are some open-source datasets that I find particularly useful or interesting in my research:</p>

            <h3>EEG</h3>
            <ul>

                <li>
                    <strong>Publication: “A multi-day and high-quality EEG dataset for motor imagery brain-computer interface” by Banghua Yang et al.:</strong>
                    DOI: <a href="https://doi.org/10.1038/s41597-025-04826-y">10.1038/s41597-025-04826-y</a>
                    <strong>Link: <a href="https://plus.figshare.com/articles/dataset/Brain_Computer_Interface_Motor_Imagery-EEG_Dataset/22671172?file=51001884">https://plus.figshare.com/articles/dataset/Brain_Computer_Interface_Motor_Imagery-EEG_Dataset/22671172?file=51001884</a>
                    <br><small><strong>Notes: </strong>A Motor-Imagery-(MI)-related EEG dataset, which can be used for training and testing of MI-based BCI deep learning models.</small>
                </li>


            </ul>

            <h3>fMRI</h3>
            <ul>

                <li>
                    <strong>Publication: “An fMRI dataset for investigating language control and cognitive control in bilinguals” by Tingting Guo et al.:</strong>
                    DOI: <a href="https://doi.org/10.1038/s41597-025-05245-9">10.1038/s41597-025-05245-9</a>
                    <strong>Link: <a href="https://openneuro.org/datasets/ds005455/versions/1.1.5">https://openneuro.org/datasets/ds005455/versions/1.1.5</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>


                <li>
                    <strong>Publication: “A natural language fMRI dataset for voxelwise encoding models” by Amanda LeBel et al.:</strong>
                    DOI: <a href="https://doi.org/10.1038/s41597-023-02437-z">10.1038/s41597-023-02437-z</a>
                    <strong>Link: <a href="https://openneuro.org/datasets/ds003020/versions/2.0.0">https://openneuro.org/datasets/ds003020/versions/2.0.0</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>


                <li>
                    <strong>Publication: “An fMRI Dataset for Concept Representation with Semantic Feature Annotations” by Shaonan Wang et al.:</strong>
                    DOI: <a href="https://doi.org/10.1038/s41597-022-01840-2">10.1038/s41597-022-01840-2</a>
                    <strong>Link: <a href="https://openneuro.org/datasets/ds004301">https://openneuro.org/datasets/ds004301</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>




            </ul>


            <h3>Simultaneous EEG-fMRI</h3>
            <ul>
                <li>
                    <strong>Publication: “An open-access dataset using simultaneous EEG-fMRI” by Telesford QK et al.:</strong>
                    DOI: <a href="https://doi.org/10.1101/2022.11.23.517540">10.1016/j.neuroimage.2021.118591</a>
                    <strong>Link: <a href="https://fcon_1000.projects.nitrc.org/indi/retro/nat_view.html">https://fcon_1000.projects.nitrc.org/indi/retro/nat_view.html</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>


            </ul>

            <h3>ECoG</h3>
            <ul>

                <li>
                    <strong>Publication: “The “Podcast” ECoG dataset for modeling neural activity during natural language comprehension” by Zaid Zada et al.:</strong>
                    DOI: <a href="https://doi.org/10.1038/s41597-025-05462-2">10.1038/s41597-025-05462-2</a>
                    <strong>Link: <a href="https://openneuro.org/datasets/ds005574/versions/1.0.2">https://openneuro.org/datasets/ds005574/versions/1.0.2</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>

            </ul>

            <h3>fNIRS</h3>
            <ul>


            </ul>

            <h3>DWI</h3>
            <ul>


            </ul>
        
        
        </section>



        <section id="tools">
            <h2>Open-source Tools</h2>
            <p>Here I list some useful tools or websites that are useful for neural signal processing:</p>

            <h3>EEGLAB</h3>
            <ul>
                <li>
                    <strong>Link: <a href="https://sccn.ucsd.edu/eeglab/">https://sccn.ucsd.edu/eeglab/</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>
            </ul>


            <h3>BRAINFLOW</h3>
            <ul>
                <li>
                    <strong>Link: <a href="https://brainflow.org/">https://brainflow.org/</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>
            </ul>

            <h3>BRAINSTORM</h3>
            <ul>
                <li>
                    <strong>Link: <a href="https://neuroimage.usc.edu/brainstorm/Introduction">https://neuroimage.usc.edu/brainstorm/Introduction</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>
            </ul>

            <h3>rsHRF</h3>
            <ul>
                <li>
                    <strong>“rsHRF: A toolbox for resting-state HRF estimation and deconvolution” by Guo-Rong Wu et al.:</strong>
                    DOI: <a href="https://doi.org/10.1016/j.neuroimage.2021.118591">10.1016/j.neuroimage.2021.118591</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>
            </ul>

            <h3>OpenBCI</h3>
            <ul>
                <li>
                    <strong>Link: <a href="https://openbci.com/">https://openbci.com/</a>
                    <br><small><strong><!-- [Notes to be added in the next update] --></strong></small>
                </li>
            </ul>

        
        
        </section>

        <section id="ideas">
            <h2>Sparks of Thought</h2>
            <p> I believe technology's most meaningful progress is guided by personal inspiration. Here, I summarize some divergent thoughts during my research and literature reading:</p>

            <h3></h3>
            <ul>



            </ul>

            <h3></h3>
            <ul>




            </ul>
            <!-- You can write about questions you're pondering or your views on a societal phenomenon -->
        </section>
    </div>

    <footer>
        <p>&copy; 2025 ZOU CHENGQI Kay. All rights reserved.</p>
        <p>Back to <a href="index.html">Main Page</a></p>
    </footer>
</body>
</html>
